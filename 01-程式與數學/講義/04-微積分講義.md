# 微積分講義

* 講義
    * 對話 -- https://gemini.google.com/app/294248e1ae0defa4
    * 分享 -- https://g.co/gemini/share/9f3f3689e8f9
* 歷史
    * 對話 -- https://gemini.google.com/app/0e211d488f05c351
    * 分享 -- https://gemini.google.com/share/815ab147ca5b

## Q: 微積分是什麼？如何用 python 寫出微分和積分的函數？

### 微積分是什麼？

微積分（Calculus）是數學的一個分支，主要研究**變化**的學問。它有兩個核心概念：

  * **微分（Differentiation）**：研究**瞬時變化率**，也就是一個函數在某一點的斜率。你可以想像成，當你在開車時，你的**瞬時速度**就是你行駛距離對時間的微分。
  * **積分（Integration）**：研究**累積效應**，也就是曲線下的面積。你可以想像成，如果你知道你在每個時刻的瞬時速度，就可以透過積分計算出你**累積行駛的總距離**。

這兩個概念互為**反運算**，也就是說，積分是微分的逆運算。微積分廣泛應用於物理、工程、經濟、統計學和電腦科學等領域。

-----

### 如何用 Python 寫出微分和積分的函數？

在 Python 中，我們通常不會從零開始手動寫出複雜的微積分運算函數，而是會利用強大的科學計算函式庫，例如 **SymPy** 和 **SciPy**。

#### SymPy：符號運算

**SymPy** 是一個用於**符號數學**的 Python 函式庫，它能處理數學表達式，就像你在紙上寫方程式一樣。它非常適合用來求出函數的**解析解**（也就是通用的公式）。

**1. 微分 (SymPy)**

我們可以使用 `sympy.diff()` 函數來求導。

```python
import sympy
from sympy import symbols, sin, cos

# 定義符號變數
x = symbols('x')

# 定義函數 f(x)
f_x = x**2 + 2*x + 1

# 進行微分
derivative_f_x = sympy.diff(f_x, x)

print(f"函數 f(x) = {f_x} 的微分是：{derivative_f_x}")
# 輸出：函數 f(x) = x**2 + 2*x + 1 的微分是：2*x + 2
```

**2. 積分 (SymPy)**

我們可以使用 `sympy.integrate()` 函數來求不定積分或定積分。

```python
import sympy
from sympy import symbols

# 定義符號變數
x = symbols('x')

# 定義函數 g(x)
g_x = 3*x**2

# 求不定積分
integral_g_x = sympy.integrate(g_x, x)

print(f"函數 g(x) = {g_x} 的不定積分是：{integral_g_x}")
# 輸出：函數 g(x) = 3*x**2 的不定積分是：x**3

# 求定積分（從 0 到 2）
definite_integral = sympy.integrate(g_x, (x, 0, 2))

print(f"函數 g(x) = {g_x} 在 [0, 2] 上的定積分是：{definite_integral}")
# 輸出：函數 g(x) = 3*x**2 在 [0, 2] 上的定積分是：8
```

-----

#### SciPy：數值運算

**SciPy** 是一個用於**數值計算**的 Python 函式庫，它不求出解析解，而是透過數值方法來近似計算結果。這在處理複雜或沒有解析解的函數時特別有用。

**1. 數值微分 (SciPy)**

`scipy.misc.derivative` 函數可以計算指定點的微分值。

```python
import numpy as np
from scipy.misc import derivative

# 定義一個 Python 函數
def h(x):
    return x**2 + 2*x + 1

# 在 x=3 這個點上進行數值微分
# dx 是變化量，數值越小，結果越精確
derivative_at_3 = derivative(h, 3.0, dx=1e-6)

print(f"函數 h(x) 在 x=3 的微分值是：{derivative_at_3}")
# 輸出：函數 h(x) 在 x=3 的微分值是：8.000000000000002
# 注意：解析解為 2*x+2，代入 x=3 得到 8，數值解非常接近。
```

**2. 數值積分 (SciPy)**

`scipy.integrate` 模組提供了多種數值積分方法。其中 `quad` 函數常用於計算一維定積分。

```python
import numpy as np
from scipy.integrate import quad

# 定義一個 Python 函數
def i(x):
    return np.sin(x)

# 計算函數 i(x)=sin(x) 從 0 到 pi 的定積分
result, error = quad(i, 0, np.pi)

print(f"函數 i(x)=sin(x) 在 [0, pi] 上的定積分是：{result}")
# 輸出：函數 i(x)=sin(x) 在 [0, pi] 上的定積分是：2.0000000000000004
# 注意：解析解為 -cos(x)，代入 [0, pi] 得到 -cos(pi) - (-cos(0)) = 1 - (-1) = 2。
```

總結來說，當你需要得到**通用的數學公式**時，使用 **SymPy** 會更合適；而當你只需要計算**特定點或區間的數值**時，**SciPy** 則是更有效率的選擇。

## Q: 如果不依賴套件，而是自行撰寫程式，那應該怎麼實作微分和積分？這和微積分的數學定義如何對應起來？

如果不依賴外部套件，自行從頭實作微積分的程式，我們需要回歸到微積分最核心的**數學定義**來進行數值上的近似。

-----

### 微分：極限的數值近似

微分的數學定義是「函數在某一點的瞬時變化率」，其核心概念是**極限**。

**數學定義**：
一個函數 $f(x)$ 在點 $x$ 的微分 $f'(x)$ 定義為：
$$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h}$$

這個公式表達的是，當 $h$ 趨近於零時，函數值的變化量 $(f(x+h) - f(x))$ 除以自變數的變化量 $h$ 所得到的比值。

**程式實作**：
在程式中，我們無法讓 $h$ 真正地趨近於零，但可以選取一個非常小的正數，比如 $h=0.000001$，來近似這個極限。這個方法稱為**數值微分**。

```python
def differentiate(f, x, h=1e-6):
    """
    對函數 f(x) 在點 x 進行數值微分。
    
    Args:
        f (function): 要微分的函數。
        x (float): 要計算微分的點。
        h (float): 一個非常小的數，用來近似極限。

    Returns:
        float: 在點 x 的近似微分值。
    """
    return (f(x + h) - f(x)) / h

# 範例：對 f(x) = x^2 進行微分
def f(x):
    return x**2

# 在 x=3 的點上進行微分
x_val = 3
derivative_at_3 = differentiate(f, x_val)

print(f"函數 f(x) = x^2 在 x={x_val} 的微分近似值為: {derivative_at_3}")
# 理論值為 f'(x) = 2x，在 x=3 時為 6。程式結果會非常接近 6。
```

**對應關係**：程式中的 `(f(x + h) - f(x)) / h` 正是對數學定義中 $\\frac{f(x+h) - f(x)}{h}$ 部分的直接翻譯。而程式中的**固定小數值** `h` 則是用來模擬數學定義中 `$h \to 0$` 這個極限過程。

-----

### 積分：黎曼和的數值近似

積分的數學定義是「曲線下的面積」，其核心概念是將面積分割為無數個**非常小的矩形**，然後將它們的面積加總起來。這個方法稱為**黎曼和（Riemann Sum）**。

**數學定義**：
函數 $f(x)$ 在區間 $[a, b]$ 上的定積分定義為：
$$\int_{a}^{b} f(x) \,dx = \lim_{n \to \infty} \sum_{i=1}^{n} f(x_i) \Delta x$$
其中 $\\Delta x = \\frac{b-a}{n}$，代表每個小矩形的寬度；$x\_i$ 則代表第 $i$ 個小矩形的高度所對應的點。

**程式實作**：
在程式中，我們無法使用無限多個矩形，但可以將區間 $[a, b]$ 分割成一個有限但足夠大的數 $n$ 個小矩形。當 $n$ 越大，矩形的總面積就越接近真正的積分值。

```python
def integrate(f, a, b, n=10000):
    """
    對函數 f(x) 在區間 [a, b] 進行數值積分（黎曼和方法）。
    
    Args:
        f (function): 要積分的函數。
        a (float): 積分區間的起點。
        b (float): 積分區間的終點。
        n (int): 將區間分割的矩形數量。

    Returns:
        float: 積分的近似值。
    """
    dx = (b - a) / n  # 計算每個小矩形的寬度
    total_area = 0
    
    # 遍歷所有矩形，並將面積加總
    for i in range(n):
        x_i = a + i * dx  # 選擇矩形左上角的 x 座標作為高度點
        total_area += f(x_i) * dx  # 面積 = 高度 * 寬度
        
    return total_area

# 範例：對 f(x) = x^2 在 [0, 2] 區間上積分
def g(x):
    return x**2

integral_val = integrate(g, 0, 2)

print(f"函數 g(x) = x^2 在區間 [0, 2] 的積分近似值為: {integral_val}")
# 理論值為 8/3 ≈ 2.6666...。程式結果會非常接近這個值。
```

**對應關係**：程式中的 `dx = (b - a) / n` 對應於數學定義中的 $\\Delta x$。`for` 迴圈中的 `total_area += f(x_i) * dx` 則是對應於數學定義中的求和符號 $\\sum$。程式中的**固定大數值** `n` 則是用來模擬數學定義中 `$n \to \infty$` 這個極限過程。

透過這兩個簡單的實作，我們可以清楚地看到微積分的數學概念如何被轉換為程式碼中的數值近似運算。這正是科學計算中常用的基本思想。

## Q: 上述的數值微分和自動微分的微分，有何不同？請用上述程式與 pytorch 自動微分進行對照說明

-----

### 數值微分與自動微分的差異

你問了一個非常好的問題，這兩者是現代機器學習和科學計算中非常核心的概念。簡單來說：

  * **數值微分 (Numerical Differentiation)**：這是我們上一封回覆中自己寫的程式，它**使用數學公式進行近似計算**。它透過計算兩個非常接近的點上的斜率來估計微分值。
  * **自動微分 (Automatic Differentiation)**：這是像 PyTorch 或 TensorFlow 這樣框架所使用的技術，它**透過追蹤和應用鏈式法則來精確計算**微分，而不是近似。

這兩者的主要差異在於**精準度**和**運算方式**。

-----

### 數值微分的限制（以你寫的程式為例）

```python
def differentiate(f, x, h=1e-6):
    return (f(x + h) - f(x)) / h
```

這個方法的優點是**簡單直觀**，完全符合微積分的定義。但它有兩個主要缺點：

1.  **精準度問題**：由於我們無法讓 $h$ 真正趨近於零，我們選取一個固定的小數值。這會導致**捨入誤差（Round-off Error）**，特別是當 $h$ 過小或過大時。
2.  **效率問題**：要計算一個點的微分，我們需要呼叫函數兩次：一次是 `f(x+h)`，一次是 `f(x)`。如果函數有很多輸入變數，或者我們的模型非常複雜（例如一個深度神經網絡），這將會是巨大的計算負擔。

這就像你在地圖上測量一個城市的坡度，你只能在兩個相隔很近的點之間量測，結果會是個近似值，而且你每換一個點就要重新量一次。

-----

### 自動微分的強大（以 PyTorch 為例）

**自動微分**不是**近似**，而是**精確**的。它依賴一個核心思想：所有複雜的數學運算，無論多麼龐大，都可以被分解成一系列基本的、已知的運算（例如加法、乘法、sin、cos 等）。

PyTorch 會在執行**前向傳播（Forward Pass）時，自動建立一個運算圖。這個圖記錄了所有運算和它們之間的依賴關係。當我們需要計算梯度（微分）時，PyTorch 會進行反向傳播（Backward Pass）**，從最終輸出開始，沿著運算圖反向追蹤，並使用\*\*鏈式法則（Chain Rule）\*\*來計算每個中間變數的梯度。

**PyTorch 自動微分範例**：

```python
import torch

# 啟用自動微分功能
x = torch.tensor(3.0, requires_grad=True)

# 定義函數 f(x) = x^2，PyTorch 會自動建立運算圖
y = x**2

# 進行反向傳播，計算 y 對 x 的梯度
y.backward()

# 取得梯度值
print(f"y 對 x 的梯度（dy/dx）為: {x.grad}")
# 輸出：y 對 x 的梯度（dy/dx）為: 6.0
```

**與數值微分的對照說明**：

| 特性 | 數值微分 (你寫的程式) | 自動微分 (PyTorch) |
| :--- | :--- | :--- |
| **運算方式** | 透過**近似**計算兩個點的斜率 | 透過**精確**的數學運算（鏈式法則） |
| **精準度** | 存在捨入誤差，結果是**近似值** | **精確**計算，無捨入誤差 |
| **效率** | 每次計算都需要重新執行函數，效率低 | 只需一次前向傳播和一次反向傳播 |
| **適用範圍** | 簡單函數、少量變數 | 複雜的、多變數的函數（如深度學習模型） |

**總結來說**：當你使用 PyTorch 時，你不需要擔心自己去寫一個 `differentiate` 函數。框架會自動為你處理梯度計算，而且是以一種**更快、更準確**的方式。這也是為什麼自動微分是機器學習的基石，它使得訓練擁有數百萬甚至數十億參數的神經網絡成為可能。

## Q: 泰勒展開式是什麼？為何能用來逼近函數？

### 泰勒展開式是什麼？

泰勒展開式（Taylor Series）是一種數學工具，它的核心思想是：**用多項式函數來逼近一個複雜或任意的函數**。

簡單來說，一個函數在某個點附近的行為，可以用一系列的**多項式**來精準描述。這個多項式包含函數在該點的**一階導數、二階導數、三階導數**…等等，一直到無窮階。

這個多項式的公式是：
$$f(x) = f(a) + \frac{f'(a)}{1!}(x-a) + \frac{f''(a)}{2!}(x-a)^2 + \frac{f'''(a)}{3!}(x-a)^3 + \cdots$$
其中：
* $f(x)$ 是我們要逼近的函數。
* $a$ 是我們選定的**展開點**，也就是多項式「逼近」的中心點。
* $f'(a), f''(a), f'''(a)$ 分別是函數在點 $a$ 的一階、二階、三階…導數。

---

### 為何能用來逼近函數？

泰勒展開式之所以能逼近函數，是因為**多項式具有極佳的性質**，它們在電腦中很容易計算，並且**能夠模仿其他函數的局部行為**。

我們可以從泰勒展開式的前幾項來看這個「逼近」的過程：

1.  **零階逼近：一個常數**
    * $f(x) \approx f(a)$
    * 這只是一個**常數**，代表函數在展開點 $a$ 的值。這條水平線只能在 $a$ 點附近與函數值「相交」，但完全無法描述函數的變化趨勢。

2.  **一階逼近：一條直線**
    * $f(x) \approx f(a) + \frac{f'(a)}{1!}(x-a)$
    * 這是一條**切線**，它不僅在 $a$ 點與函數值相同，還擁有與函數在該點**相同的一階斜率**。這條直線能非常準確地描述函數在 $a$ 點**極小範圍**內的趨勢。

3.  **二階逼近：一個二次曲線**
    * $f(x) \approx f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2$
    * 這個二次多項式不僅在 $a$ 點與函數值、斜率相同，還擁有**相同的二階曲率**（也就是**彎曲程度**）。這使得它在 $a$ 點附近對函數的逼近更加精準。

當我們不斷加入更高階的項時，這個多項式會**繼承**越來越多原函數的特性（如三階項會繼承三階導數的特性），這使得它在**越來越大的範圍內**都能與原函數高度重合。

**想像一下**：你在一條彎曲的公路上開車。
* **零階逼近**就像你只知道你現在的經緯度位置。
* **一階逼近**就像你還知道你現在車頭的方向（斜率）。
* **二階逼近**就像你更進一步知道這條路是左轉還是右轉、轉彎的幅度有多大（曲率）。

泰勒展開式就是將所有這些**局部資訊**（函數值、斜率、曲率…）無限地加總起來，最終描繪出函數的完整樣貌。當展開的項數足夠多時，多項式就能在一個相當大的區間內，以驚人的精準度來逼近原函數。這在物理學、工程學和機器學習等領域都有廣泛應用，例如在梯度下降法中，我們常常會用到一階或二階的泰勒展開式來近似目標函數。

