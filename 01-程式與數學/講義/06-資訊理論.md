# 資訊理論講義

* 講義
    * 對話 -- https://gemini.google.com/app/08c26d5665de8c33
    * 分享 -- https://g.co/gemini/share/a4836e62204f

* 補充
    * 對話 -- https://gemini.google.com/app/1b58a6cac773ade9
    * 分享 -- https://gemini.google.com/share/bc263684538c

* 歷史
    * 對話 -- https://gemini.google.com/app/50ceecc56dd0faa6
    * 分享 -- https://gemini.google.com/share/11898926fc59

* 習題
    * 對話 -- https://gemini.google.com/app/32ca6a3278d7b321
    * 分享 -- 不公開



## Q: 資訊理論是什麼？熵 entropy 的意義又是什麼？

資訊理論是一門用數學方法來量化、儲存、傳輸和處理資訊的學科。它的核心目標是研究如何有效率且準確地在通訊系統中傳遞訊息。

---

### 熵 (Entropy) 的意義

在資訊理論中，**熵**是一個非常重要的概念，它用來衡量一個訊息或事件的**不確定性 (uncertainty)** 或**資訊量 (information content)**。這個概念最早由克勞德·夏農 (Claude Shannon) 提出，因此也常被稱為「夏農熵」。

想像一個簡單的例子：

* **情況一：** 某人告訴你「太陽會從東邊升起」。這句話的資訊量很低，因為這是一個確定性極高的事件，你幾乎不需要這條訊息。它的熵值接近零。
* **情況二：** 某人告訴你「明天台北下雨」。這句話的資訊量相對較高，因為明天下雨並不是百分之百確定的事，這條訊息消除了你對天氣不確定性的一部分。它的熵值會比情況一高。
* **情況三：** 某人告訴你「你買的彩券中了頭獎」。這句話的資訊量非常高，因為這是極度不可能發生的事件。這個訊息大大降低了你對於「是否中獎」的不確定性，因此它的熵值非常高。

總結來說，熵的意義可以歸納為：

* **不確定性：** 某個事件或訊息**越不確定**、發生的可能性**越低**，它的熵就**越高**。
* **資訊量：** 熵越高，表示這條訊息所包含的資訊量越大，能夠消除的不確定性也越多。

因此，資訊理論的許多應用，例如資料壓縮，都是基於「降低熵」這個概念。透過移除訊息中重複或可預測的部分，我們可以用更少的位元 (bit) 來表示相同的資訊，達到壓縮的目的。

## Q: 請寫出『熵，交叉熵，KL 散度，互熵』的數學式與定義，並說明其意義

### 熵 (Entropy)

**定義：**
熵是衡量一個隨機變數或資訊源不確定性的數學量。一個事件發生的機率越低，其所包含的資訊量就越大。熵是所有可能事件的資訊量期望值。

**數學式：**
對於離散隨機變數 $X$，$p(x)$ 是 $X$ 取特定值 $x$ 的機率，其熵 $H(X)$ 定義為：
$H(X) = - \sum_{x \in X} p(x) \log_2 p(x)$

**意義：**
* **衡量不確定性：** 熵值越大，表示該隨機變數的不確定性越高。例如，一個公平的硬幣（正反面機率各為 0.5）比一個不公平的硬幣（正反面機率分別為 0.99 和 0.01）具有更高的熵。
* **衡量編碼長度：** 在資訊編碼中，熵代表對該資訊源進行無損壓縮時，每個符號所需的平均最小編碼位元數。

---

### 交叉熵 (Cross-Entropy)

**定義：**
交叉熵是用來衡量兩個機率分佈之間差異的指標。它常被用於機器學習中，特別是分類任務，用來評估模型的預測分佈 $q(x)$ 與真實分佈 $p(x)$ 之間的差異。

**數學式：**
對於兩個機率分佈 $p(x)$ 和 $q(x)$，它們之間的交叉熵 $H(p, q)$ 定義為：
$H(p, q) = - \sum_{x \in X} p(x) \log_2 q(x)$

**意義：**
* **模型誤差評估：** 在分類問題中，$p(x)$ 代表真實標籤的分佈（通常是 one-hot 編碼），而 $q(x)$ 則是模型預測的分佈。交叉熵越小，表示模型預測結果越接近真實結果，代表模型效能越好。
* **與熵的關係：** 交叉熵可以分解為熵與 KL 散度之和，即 $H(p, q) = H(p) + D_{KL}(p || q)$。

---

### KL 散度 (Kullback-Leibler Divergence)

**定義：**
KL 散度，又稱相對熵（Relative Entropy），是用來衡量一個機率分佈 $q(x)$ 與另一個基準機率分佈 $p(x)$ 之間的差異程度。

**數學式：**
對於兩個機率分佈 $p(x)$ 和 $q(x)$，從 $p$ 到 $q$ 的 KL 散度 $D_{KL}(p || q)$ 定義為：
$D_{KL}(p || q) = \sum_{x \in X} p(x) \log_2 \frac{p(x)}{q(x)}$

**意義：**
* **衡量分佈差異：** KL 散度可以理解為「用 $q$ 來近似 $p$ 所付出的代價」。當 $p(x)$ 和 $q(x)$ 完全相同時，KL 散度為 0。當它們的分佈差異越大，KL 散度值也越大。
* **不對稱性：** 值得注意的是，KL 散度不是一個對稱的指標，即 $D_{KL}(p || q) \neq D_{KL}(q || p)$。這意味著從 $p$ 到 $q$ 的差異與從 $q$ 到 $p$ 的差異是不同的。

---

### 互資訊 (Mutual Information)

**定義：**
互資訊是用來衡量兩個隨機變數之間相互依賴程度的指標。它量化了在已知其中一個變數的條件下，另一個變數的不確定性減少了多少。

**數學式：**
對於兩個隨機變數 $X$ 和 $Y$，其互資訊 $I(X; Y)$ 定義為：
$I(X; Y) = \sum_{x \in X} \sum_{y \in Y} p(x, y) \log_2 \frac{p(x, y)}{p(x)p(y)}$

**意義：**
* **衡量關聯性：** 如果 $X$ 和 $Y$ 之間存在強烈關聯，當我們知道 $X$ 的值時，對 $Y$ 的不確定性就會大幅降低，此時互資訊值會很高。如果 $X$ 和 $Y$ 彼此獨立，知道 $X$ 對 $Y$ 的不確定性沒有影響，互資訊值為 0。
* **與熵的關係：** 互資訊可以表示為 $I(X; Y) = H(X) - H(X|Y)$，其中 $H(X|Y)$ 是條件熵，代表在已知 $Y$ 的情況下 $X$ 的不確定性。這個公式直觀地說明了互資訊就是**原本的不確定性**（$H(X)$）減去**在得知 $Y$ 後剩下的不確定性**（$H(X|Y)$）。

## Q: 為何深度學習要用交叉熵 (Cross Entropy) 來當損失函數

在深度學習中，**交叉熵**之所以被廣泛用作損失函數，特別是在分類任務中，主要原因有以下幾點：

---

### 1. 衡量機率分佈的「距離」

交叉熵能有效地衡量兩個機率分佈之間的相似程度。

* 在分類問題中，我們通常將**真實標籤 (Ground Truth)** 表示為一個機率分佈。例如，如果一張圖片是貓，真實標籤可以表示為 [1, 0, 0]，代表 100% 是貓，0% 是狗，0% 是鳥。這是一個**真實分佈 (True Distribution)** $p(x)$。
* 神經網路的輸出，通常經過 Softmax 函式，也會得到一個機率分佈。例如，模型預測圖片是貓、狗、鳥的機率分別為 [0.7, 0.2, 0.1]。這是一個**預測分佈 (Predicted Distribution)** $q(x)$。

交叉熵損失函數的作用就是計算這兩個分佈之間的差異。當模型預測的分佈 $q(x)$ 越接近真實分佈 $p(x)$ 時，交叉熵的值就越小，表示模型的預測越準確。因此，最小化交叉熵損失函數，就是在訓練模型去學習如何讓預測分佈更貼近真實分佈。

---

### 2. 具有優良的梯度特性

交叉熵損失函數在訓練過程中提供了穩定且有效的梯度（Gradient），這對於梯度下降等優化演算法至關重要。

* **避免梯度消失：** 許多其他損失函數，例如均方誤差 (Mean Squared Error, MSE)，在用於分類任務時，當模型的預測值與真實值相差很大時，梯度會變得非常小，導致訓練停滯（即**梯度消失**）。
* 交叉熵則不同。其梯度與神經網路的輸出值有直接關係，當預測值與真實值相差越大，其梯度也越大，這讓模型能夠快速地朝著正確的方向調整權重。這使得訓練過程更穩定、收斂更快。

舉例來說，當模型對一個正確的類別給出接近於 0 的預測機率時（表示模型極度不確定或錯誤），交叉熵損失會變得非常大，其梯度也會非常陡峭，促使模型進行大幅度調整，從而快速糾正錯誤。

---

### 3. 專為分類任務設計

交叉熵損失函數的設計初衷就是為了處理機率和分類問題。

* **適用於多分類：** 交叉熵可以輕鬆擴展到多分類問題，它能同時考慮所有類別的機率分佈，而不會像二分類一樣只關注一個類別。
* **物理意義明確：** 損失值的高低直接反映了模型預測與真實情況之間的「不確定性」或「資訊量差異」。這使得模型訓練的目標變得非常直觀：降低預測結果中的不確定性，使其與真實世界保持一致。

總結來說，交叉熵之所以成為深度學習分類任務的損失函數首選，是因為它不僅能夠**量化模型的預測誤差**，還能**提供穩定的梯度**以有效率地訓練模型，並且其背後的**數學原理與分類任務的核心目標高度契合**。

## Q: 霍夫曼編碼法是什麼？和熵有何關係？

### 霍夫曼編碼法 (Huffman Coding)

**霍夫曼編碼法**是一種用於無損資料壓縮的演算法，由大衛·霍夫曼 (David Huffman) 在 1952 年提出。它的核心思想是：

* **給予高頻率的字元（或資料）較短的編碼。**
* **給予低頻率的字元較長的編碼。**

這樣做的好處是，在傳輸或儲存資料時，因為頻繁出現的符號使用了更短的位元數，整體的平均編碼長度就會大幅縮短，達到壓縮的目的。霍夫曼編碼所產生的編碼是**前綴碼 (prefix code)**，這意味著任何一個編碼都不是另一個編碼的前綴，因此解碼時不會產生歧義。

舉例來說，假設我們有一段文字，其中包含以下四個字元及其出現頻率：

* A: 50%
* B: 20%
* C: 15%
* D: 15%

如果我們使用定長編碼（例如，每個字元都用 2 位元），總共需要 $2$ 位元。但如果使用霍夫曼編碼，可能會得到類似以下的變長編碼：

* A: 0
* B: 10
* C: 110
* D: 111

這樣，我們就可以用平均更少的位元來表示整段文字，實現資料壓縮。

---

### 霍夫曼編碼與熵 (Entropy) 的關係

**熵**在資訊理論中代表一個資料源的**最小平均編碼長度**，也就是資訊的「理論極限」。霍夫曼編碼法與熵的關係是：

1. **霍夫曼編碼逼近熵的極限：**
   霍夫曼編碼所產生的平均編碼長度非常接近該資訊源的熵值。換句話說，霍夫曼編碼是一種「接近最佳化」的資料壓縮方法。它能夠產生一個**幾乎等於**（或非常接近）理論上最小編碼長度的編碼。

2. **為什麼是「幾乎」？**
   * **等號成立條件：** 當所有符號的機率都是 2 的負整數次方時，霍夫曼編碼所產生的平均編碼長度會**完全等於**熵值。
   * **不等號條件：** 在實際應用中，符號的機率通常不是 2 的負整數次方。這時，霍夫曼編碼的平均編碼長度會略大於熵值，但差異通常很小。

總結來說，熵為我們提供了一個**理論上的最小壓縮極限**，而霍夫曼編碼法則是**實現這個極限的一種高效演算法**。它利用了符號的統計特性，透過給予高頻率符號短編碼來逼近這個理論極限，從而實現了有效的無損資料壓縮。