## 第七章：大型語言模型的後訓練技術

在預訓練 (Pre-training) 階段之後，模型雖然習得了海量的知識，但尚未能精確地遵循人類指令或進行複雜的邏輯推理。本章將探討如何透過後訓練 (Post-training) 技術，特別是結合強化學習 (Reinforcement Learning, RL) 與偏好優化，提升模型的對齊 (Alignment) 能力與推理水平。

---

### 7.1 基於規則的獎勵模型 (Rule-based Reward Model) 與過程獎勵模型 (Process Reward Model, PRM)

在強化學習的框架中，獎勵函數 (Reward Function) 定義了模型目標。傳統的結果獎勵模型 (Outcome Reward Model, ORM) 僅根據最終答案的正確與否給予回饋，但在處理多步驟推理問題時，這往往不足以捕捉中間邏輯的錯誤。

* **基於規則的獎勵模型 (Rule-based Reward Model, RRM)**：
這類模型不依賴神經網路來評分，而是透過預定義的規則（如正則表達式、編譯器輸出或數學等價性檢查）來給予獎勵。其優點是客觀且一致，特別適用於程式碼生成或具有標準答案的數學題。
* **過程獎勵模型 (Process Reward Model, PRM)**：
與只看結果的 ORM 不同，PRM 會對推理路徑中的每一個中間步驟 (Intermediate Step) 進行評價。

**數學定義：**
假設一個推理任務包含  個步驟 。ORM 的獎勵僅取決於最終狀態：



而 PRM 則為每個步驟提供即時回饋 ：



其中  為折扣因子 (Discount Factor)。透過這種方式，模型能學習到哪些具體的邏輯轉折點是正確的，顯著提升了處理複雜問題的穩定性。

---

### 7.2 純強化學習 (Pure RL) 的湧現行為：以 DeepSeek-R1-Zero 為例

DeepSeek-R1-Zero 展示了一種極端的後訓練範式：在沒有初始監督式微調 (Supervised Fine-Tuning, SFT) 的情況下，直接對預訓練基座模型進行純強化學習。

**數學機制：**
模型通常使用群體相對策略優化 (Group Relative Policy Optimization, GRPO)。在給定問題  下，採樣一組輸出 ，並計算其相對優勢 (Advantage)：



模型透過最大化期望獎勵來更新參數 。

**湧現行為 (Emergent Behavior)：**
在訓練過程中，模型會自發性地學會「反思」與「自我修正」。例如，當模型發現中間推理步驟  導致矛盾時，它會產生類似「等等，我剛才算錯了」的標籤，並重新嘗試新的路徑。這種行為並非由人類示範標註，而是為了獲取更高獎勵而演化出的計算策略。

---

### 7.3 直接偏好優化 (Direct Preference Optimization, DPO) 的數學優勢

傳統的基於人類回饋的強化學習 (RLHF) 需要先訓練一個獎勵模型 ，再利用 PPO 演算法優化策略 。DPO 則提出了一種封閉解 (Closed-form solution)，直接在偏好數據上優化策略。

**數學推導：**
在給定偏好對 （其中  優於 ）時，DPO 的損失函數  定義為：



其中  是參考模型， 是 Sigmoid 函數， 是控制偏差的超參數。

**優勢：**

1. **穩定性**：避開了 PPO 中不穩定的關鍵字採樣與多模型協同（演員、評論家、獎勵模型、參考模型）。
2. **效率**：將強化學習問題轉化為簡單的二元分類交叉熵問題，極大地減少了計算開銷。

---

### 7.4 拒絕採樣 (Rejection Sampling) 與冷啟動數據 (Cold-start Data) 的協同

雖然純 RL 能產生湧現行為，但若從零開始，收斂速度極慢且初期輸出混亂。實踐中常結合「冷啟動」與「拒絕採樣」來引導訓練。

* **冷啟動數據 (Cold-start Data)**：
使用少量高質量的思維鏈 (Chain of Thought, CoT) 數據進行初始 SFT，讓模型初步掌握思考的格式。
* **拒絕採樣 (Rejection Sampling)**：
1. 對於一個問題 ，使用當前模型  採樣多個候選答案 。
2. 利用獎勵模型（或規則檢查）篩選出得分最高的樣本。
3. 將這些高品質樣本回流，作為下一輪 SFT 的訓練數據。



**數學描述：**
我們可以將拒絕採樣視為對分佈  的截斷與重加權：



其中  是門檻值。這種方法能有效地將 RL 探索到的最優路徑固化到模型的權重中。

