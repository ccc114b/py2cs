## 生成式 AI：從數學原理到實踐應用

### 第三章：生成對抗網路

生成對抗網路 (Generative Adversarial Networks, GAN) 的核心思想源於對抗的概念。它由兩個神經網路組成：**生成器** (Generator, ) 與**判別器** (Discriminator, )。這兩者在訓練過程中相互競爭，最終達到一種動態平衡。

---

### 3.1 賽局理論 (Game Theory) 與納許均衡 (Nash Equilibrium)

GAN 的訓練過程可以被建模為一個**零和賽局** (Zero-sum game)。在這種賽局中，一方的收益必然等於另一方的損失。

**數學定義：**
在賽局理論中，**納許均衡** (Nash Equilibrium) 指的是在一個多玩家策略組合中，任何一個玩家在其他玩家策略不變的情況下，都無法透過單方面改變自己的策略來獲得更好的利益。

對於 GAN 而言，當生成器  產生的樣本機率分佈  完全等於真實數據的分佈 ，且判別器  無法分辨兩者（預測機率均為 ）時，系統便達到了納許均衡。此時，任何一方的單獨更新都無法進一步優化其目標函數。

---

### 3.2 生成器與判別器的極小化極大演算法 (Minimax Algorithm)

GAN 的訓練目標可以用一個目標函數  來描述。判別器試圖最大化正確分類的機率，而生成器則試圖最小化判別器正確判斷的可能。

**數學定義：**
GAN 的核心目標函數如下：


其中：

*  是來自真實分佈的數據。
*  是輸入生成器的噪聲向量，服從分佈 。
*  是生成器產生的偽造樣本。
*  是判別器判定  為真實數據的機率。

**範例：**
假設我們正在訓練一個生成手寫數字的 GAN。

1. **判別器階段**：給予真實數字圖像時， 應輸出接近 ；給予  時， 應輸出接近 。
2. **生成器階段**： 調整參數，使得  的值愈大愈好，從而「欺騙」判別器。

---

### 3.3 詹森-香農散度 (Jensen-Shannon Divergence) 的角色

在給定生成器  的情況下，最佳的判別器  可以透過對  求偏微分得到：


將此最佳化判別器代回目標函數，我們可以發現優化生成器  實際上等同於最小化真實分佈  與生成分佈  之間的**詹森-香農散度** (Jensen-Shannon Divergence, JSD)。

**數學定義：**
兩個分佈  與  的 JSD 定義為：



其中 ，而  為**庫爾貝克-萊布勒散度** (Kullback-Leibler Divergence)。JSD 的值域在  之間，當兩分佈完全重合時為 。

---

### 3.4 瓦瑟斯坦 GAN (Wasserstein GAN) 與最優傳輸理論

傳統 GAN 使用 JSD 會遇到梯度消失問題，特別是當兩個分佈完全沒有重疊時，JSD 會變成常數 。為了改善這點，引入了基於**最優傳輸** (Optimal Transport) 理論的**瓦瑟斯坦距離** (Wasserstein Distance)，又稱為**推土機距離** (Earth Mover's Distance, EMD)。

**數學定義：**
瓦瑟斯坦距離定義為將一個分佈變換為另一個分佈所需的最小傳輸代價：


在實作中，利用 Kantorovich-Rubinstein 對偶性 (Duality)，WGAN 的目標函數轉化為：



這裡的判別器 （通常稱為 **評論者** (Critic)）必須滿足 **利普希茨連續性** (1-Lipschitz continuity)。這確保了即使在分佈不重疊的情況下，梯度依然存在且平滑，大大提升了訓練的穩定性。

**範例：**
若有兩個分佈分別位於數線上的  與 。JSD 在  時恆為常數，無法提供梯度引導  趨向 ；而 ，其導數為常數，能持續引導模型優化。
