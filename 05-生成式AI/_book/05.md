## 第五章：變換器模型與注意力機制

在生成式人工智慧中，變換器 (Transformer) 模型的成功核心在於它能捕捉序列中不同位置之間的長距離依賴關係。本章將從線性代數與向量空間的角度，深入探討其核心組件。

---

### 5.1 注意力機制 (Attention Mechanism) 的向量空間運算

**注意力機制** 的本質是將一個查詢向量與一組鍵向量進行比對，藉此計算出各個值向量的加權總和。

**數學定義：**
假設輸入序列由一組向量組成，我們將其線性映射到三個不同的子空間：查詢 (Query, )、鍵 (Key, ) 與值 (Value, )。給定一個查詢向量  以及一組鍵向量  和值向量 ，注意力運算定義為：

其中權重  是透過相似度函數計算並經過歸一化得到的標量：

這代表輸出向量是值向量空間中的線性組合，其係數取決於查詢與鍵在向量空間中的相關程度。

---

### 5.2 縮放點積注意力 (Scaled Dot-Product Attention) 的解析

為了提高運算效率並保持數值穩定性，實務上最常採用的是 **縮放點積注意力**。

**數學定義：**
若將多個查詢組合成矩陣 ，鍵與值分別為  與 ，其運算式為：

**範例：**
當兩個向量  與  的維度  很大時，點積  的變異數會隨之增加，導致 Softmax 函數進入梯度極小的區域。透過除以  進行縮放 (Scaling)，可以將點積的變異數控制在  附近，確保梯度傳播穩定。

---

### 5.3 位置編碼 (Positional Encoding) 的正餘弦性質

由於注意力機制本身是對稱的（即交換輸入順序，輸出結果的集合不變），變換器模型需要 **位置編碼** 來引入序列的順序資訊。

**數學定義：**
位置編碼採用不同頻率的正弦 (Sine) 與餘弦 (Cosine) 函數。對於位置  與維度指標 ：

**範例：**
這種設計具備線性變換的性質。對於任何固定的偏移量 ， 可以表示為  的線性函數。這使得模型能夠輕易地學習到相對位置關係，因為：



這類三角恆等式保證了位置之間的平移不變性。

---

### 5.4 多頭注意力 (Multi-Head Attention) 的並行表示

**多頭注意力** 允許模型在不同的投影子空間中同時關注來自不同位置的資訊。

**數學定義：**
將  分別透過  組不同的權重矩陣  進行線性變換，計算出  個「頭」 (Head)：

最後將這些頭拼接 (Concatenate) 起來，再進行一次線性變換：

**範例：**
假設我們處理一句話，一個頭可能專注於語法關係（如動詞與受詞），而另一個頭則專注於語義關聯（如代名詞所指代的對象）。透過這種並行表示 (Parallel Representation)，模型能捕捉更豐富的特徵。

