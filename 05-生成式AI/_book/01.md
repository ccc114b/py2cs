## 第一章：生成式模型基礎

這章節將探討生成式模型的數學核心，介紹如何透過機率分佈來描述數據的生成過程，並建立後續深度學習模型的理論根基。

---

### 1.1 概率生成模型 (Probabilistic Generative Models) 概述

**定義**：
給定一組觀測數據 ，生成式模型的目標是學習該數據的聯合機率分佈 (Joint Probability Distribution) ，或者在有標籤的情況下學習 。若我們能成功估計出這個分佈，就能從中進行抽樣 (Sampling)，產生與原始數據特徵相似的新樣本。

這與判別式模型 (Discriminative Models) 不同，後者僅關注給定  下  的條件機率 。

**範例**：
假設有一組二維平面上的點集，呈現高斯分佈 (Gaussian Distribution)。一個生成式模型會估計其平均值向量  與協方差矩陣 (Covariance Matrix) 。一旦參數確定，我們就能利用機率密度函數 (Probability Density Function, PDF) 產生新的座標點：


---

### 1.2 最大似然估計 (Maximum Likelihood Estimation) 與推斷

**定義**：
最大似然估計 (Maximum Likelihood Estimation, MLE) 是一種統計方法，用於估計機率模型的參數 。其核心思想是：尋找一組參數 ，使得觀測到現有數據  的可能性（即似然函數）最大化。

數學上，我們通常對似然函數取對數，將乘積轉化為求和，稱為對數似然 (Log-Likelihood)：


**範例**：
考慮一次伯努利試驗 (Bernoulli trial)，例如投擲一枚硬幣  次，出現正面的次數為 。我們想估計正面出現的機率 。
其似然函數為 。
取對數後求導並令其為零：



解得 ，這符合我們直觀的頻率比例。

---

### 1.3 潛在變量模型 (Latent Variable Models)

**定義**：
在許多複雜數據中，觀測到的變數  往往是由某些不可觀測的潛在變量 (Latent Variables)  所驅動的。潛在變量模型試圖透過  來捕捉數據的底層結構或低維度特徵。

根據全機率公式，觀測數據的邊際分佈 (Marginal Distribution) 可以表示為：



其中  是先驗分佈 (Prior Distribution)， 是生成過程（或稱解碼器）。

**範例**：
在高斯混合模型 (Gaussian Mixture Model, GMM) 中， 是觀測到的數據點，而  代表該點屬於哪一個高斯成分（類別）。雖然我們不知道每個點具體來自哪個類別，但透過推斷 ，我們可以生成具有特定聚類特徵的新數據。

---

### 1.4 資訊理論 (Information Theory) 與熵的概念

**定義**：
資訊理論提供了衡量隨機變數不確定性的工具。**熵 (Entropy)** 是描述一個隨機變數  平均資訊量的度量：



在生成模型中，我們常使用 **克拉伯－萊布勒散度 (Kullback-Leibler Divergence, KL Divergence)** 來衡量兩個機率分佈  與  之間的差異：



當  與  完全相同時，。

**範例**：
在訓練生成對抗網路 (GAN) 或變分自編碼器 (VAE) 時，我們本質上是在最小化真實數據分佈  與模型生成分佈  之間的距離。例如，VAE 的損失函數中包含一項 KL 散度，用來約束潛在空間的分佈趨近於標準常態分佈 。
