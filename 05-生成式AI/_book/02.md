## 第二章：變分自編碼器 (Variational Autoencoders, VAE)

變分自編碼器是一種強大的生成模型，其核心思想是將高維的觀測數據映射到一個低維的連續空間，並從中學習數據的機率分佈。

---

### 2.1 變分推斷 (Variational Inference) 的數學結構

在生成模型中，我們假設觀測數據  是由某種隱藏的潛在變量 (Latent Variable)  所產生的。根據貝氏定理 (Bayes' Theorem)，後驗機率 (Posterior Probability) 為：

然而，分母中的邊際似然 (Marginal Likelihood)  通常在高維空間中是無法計算的 (Intractable)。變分推斷的目標是尋找一個簡單的分佈 （例如高斯分佈），來逼近真實但不可求的後驗分佈 。

我們透過最小化 庫爾貝克-萊布勒散度 (Kullback-Leibler Divergence, KL Divergence) 來衡量兩個分佈之間的差異：

---

### 2.2 證據下界 (Evidence Lower Bound, ELBO) 的推導

由於直接最小化上述 KL 散度依然涉及 ，我們將其重新展開。根據對數性質，數據的邊際對數似然可以寫為：

其中  即為 **證據下界 (Evidence Lower Bound, ELBO)**。因為 KL 散度恆大於或等於 ，所以  是  的一個下限。最大化 ELBO 等同於最小化 KL 散度。

ELBO 的數學形式通常寫作：

* **第一項（重建項）**：衡量模型從潛在變量  還原回原始數據  的能力。
* **第二項（正規化項）**：衡量變分分佈  與先驗分佈 (Prior Distribution) （通常假設為標準正態分佈）之間的差異。

---

### 2.3 重參數化技巧 (Reparameterization Trick)

在訓練 VAE 時，我們需要對 ELBO 關於參數  求梯度。然而，由於  是從  中隨機採樣得到的，採樣過程本身是不可微的，這會導致隨機梯度下降 (Stochastic Gradient Descent) 無法進行。

為了克服這個問題，我們使用重參數化技巧。假設  是一個高斯分佈 ，我們可以將  表示為：

透過這種方式，隨機性被轉移到了  身上，而  對於  和  變成了可微的線性運算，從而允許誤差反向傳播 (Backpropagation) 順利通過採樣層。

---

### 2.4 潛在空間 (Latent Space) 的流形結構

VAE 的一個關鍵特性是它能學習到具有物理意義的 **潛在空間 (Latent Space)**。由於 ELBO 中 KL 散度項的約束，潛在空間不會是支離破碎的，而是呈現出一種連續且平滑的 **流形 (Manifold)** 結構。

在數學上，這意味著：

1. **語義平滑性**：在潛在空間中移動一小步，生成的樣本  只會發生細微的語義變化（例如：人臉從微笑逐漸變為嚴肅）。
2. **聚類特性**：相似特徵的數據會在潛在空間中被編碼到相近的座標區域。

這種流形結構使得我們可以在潛在空間進行向量運算（如：），實現受控的圖像生成與編輯。
