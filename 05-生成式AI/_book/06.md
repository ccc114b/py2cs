## 第六章：大型語言模型基礎與生成策略

本章將探討大型語言模型 (Large Language Model, LLM) 的核心運作機制，從機率論的基礎出發，進而研究如何透過不同的訓練與生成策略，使模型展現出流暢且具備邏輯的文本生成能力。

---

### 6.1 語言建模 (Language Modeling) 與條件機率鏈

語言建模的核心目標在於估計一個句子（或序列）出現的機率。假設一個序列由  個標記 (Token) 組成，記作 。

根據機率論中的 **乘法法則 (Chain Rule of Probability)**，我們可以將此序列的聯合機率分解為連續條件機率的乘積：

這意味著模型在預測下一個字  時，必須依賴於先前出現過的所有上下文 。在現代神經網路中，模型學習的是一個參數化函數 ，用以近似這個條件機率分佈：

---

### 6.2 自回歸生成 (Autoregressive Generation) 策略

**自回歸 (Autoregressive)** 是指模型在生成文本時，會將自己先前生成的輸出作為下一步的輸入。這是一個序列化的過程，直到模型輸出一個特殊的終止符號。

常見的生成採樣策略包括：

1. **貪婪搜尋 (Greedy Search)**：每一時間步都選擇機率最高的 Token。


2. **隨機採樣 (Random Sampling)**：根據機率分佈隨機選取，增加多樣性。
3. **核採樣 (Nucleus Sampling / Top-p Sampling)**：只從累積機率和達到  的前幾個候選字中進行採樣。

---

### 6.3 指令微調 (Instruction Fine-tuning) 的原理

預訓練模型 (Pre-trained Model) 雖然具備強大的語言預測能力，但往往只是在「續寫」文本。為了使其能理解並執行特定的人類指令，我們需要進行 **指令微調 (Instruction Fine-tuning)**。

此過程使用大量的指令數據集（例如：「請翻譯以下句子」、「請寫一段程式碼」）對模型進行監督式學習。目標函數與預訓練相同，皆為最小化負對數似然 (Negative Log-Likelihood, NLL)：

其中  是輸入指令， 是目標回答。透過此階段，模型學會了將輸入的語意對齊 (Alignment) 到人類期待的行為模式。

---

### 6.4 基於人類回饋的強化學習 (Reinforcement Learning from Human Feedback, RLHF)

儘管指令微調能讓模型學會指令，但模型產出的品質仍難以僅用交叉熵損失來衡量。**基於人類回饋的強化學習 (Reinforcement Learning from Human Feedback, RLHF)** 引入了人類的偏好來進一步優化模型。

RLHF 通常包含三個步驟：

1. **監督式微調 (SFT)**：即上述的指令微調。
2. **訓練獎勵模型 (Reward Model, RM)**：讓人類對模型的數個輸出進行排序，並訓練一個純量函數  來模擬人類的偏好。
3. **近端策略優化 (Proximal Policy Optimization, PPO)**：利用強化學習演算法，將 LLM 視為一個策略 (Policy)，以極大化預期的回饋分數：



其中第二項是 **KL 散度 (Kullback-Leibler Divergence)**，用於約束模型不要偏離原始分佈太遠，以確保生成文本的穩定性與自然度。

