import gymnasium as gym
import ale_py
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from model import SimpleDQN, get_device, extract_pong_state_from_ram

# è¨»å†Š ALE ç’°å¢ƒ
gym.register_envs(ale_py)


class RuleBasedAgent:
    """åŸºæ–¼è¦å‰‡çš„ Pong ä»£ç†ï¼šç°¡å–®è¿½çƒç­–ç•¥"""
    
    def __init__(self):
        self.actions = [0, 2, 3]  # NOOP, UP, DOWN
    
    def select_action(self, state):
        """
        ç°¡å–®è¦å‰‡ï¼šç¸½æ˜¯è¿½çƒ
        è¿”å›å‹•ä½œç´¢å¼• (0, 1, 2)
        """
        ball_x, ball_y, ball_vx, ball_vy, paddle_y, enemy_paddle_y = state
        
        # ç°¡å–®è¿½çƒé‚è¼¯
        if ball_y < paddle_y - 0.02:
            return 1  # UP
        elif ball_y > paddle_y + 0.02:
            return 2  # DOWN
        else:
            return 0  # STAY


def collect_expert_data(n_episodes=100, debug=False):
    """æ”¶é›†è¦å‰‡ä»£ç†çš„ç¶“é©—æ•¸æ“š"""
    env = gym.make("ALE/Pong-v5", render_mode=None)
    agent = RuleBasedAgent()
    
    expert_data = []
    episode_rewards = []
    
    print("ğŸ“š æ”¶é›†å°ˆå®¶æ•¸æ“šï¼ˆè¦å‰‡ä»£ç†ï¼‰...")
    
    for episode in range(n_episodes):
        obs, _ = env.reset()
        prev_ram = None
        state = extract_pong_state_from_ram(obs, prev_ram)
        
        episode_reward = 0
        done = False
        step_count = 0
        action_counts = [0, 0, 0]
        
        while not done:
            # è¦å‰‡é¸æ“‡å‹•ä½œ
            action_idx = agent.select_action(state)
            action = agent.actions[action_idx]
            action_counts[action_idx] += 1
            
            # Debug æ¨¡å¼
            if debug and episode == 0 and step_count < 10:
                print(f"\n  Step {step_count}:")
                print(f"    State: ball=({state[0]:.2f},{state[1]:.2f}), "
                      f"v=({state[2]:.3f},{state[3]:.3f}), "
                      f"paddle={state[4]:.2f}, enemy={state[5]:.2f}")
                print(f"    Action: {['STAY', 'UP', 'DOWN'][action_idx]}")
            
            # åŸ·è¡Œå‹•ä½œ
            next_obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            next_state = extract_pong_state_from_ram(next_obs, obs)
            
            # ä¿å­˜ç¶“é©—
            expert_data.append((state, action_idx))
            
            episode_reward += reward
            prev_ram = obs
            obs = next_obs
            state = next_state
            step_count += 1
        
        episode_rewards.append(episode_reward)
        
        if (episode + 1) % 10 == 0:
            avg_reward = np.mean(episode_rewards[-10:])
            print(f"  Episode {episode + 1}/{n_episodes}, "
                  f"Avg Reward: {avg_reward:.1f}, "
                  f"Actions: STAY={action_counts[0]}, UP={action_counts[1]}, DOWN={action_counts[2]}")
    
    env.close()
    
    avg_reward = np.mean(episode_rewards)
    print(f"\nâœ“ æ”¶é›†å®Œæˆï¼å…± {len(expert_data)} å€‹æ¨£æœ¬")
    print(f"  è¦å‰‡ä»£ç†å¹³å‡çå‹µ: {avg_reward:.2f}")
    
    return expert_data, avg_reward


def pretrain_from_expert(expert_data, n_epochs=10):
    """å¾å°ˆå®¶æ•¸æ“šé è¨“ç·´æ¨¡å‹"""
    device = get_device()
    
    # å‰µå»ºæ¨¡å‹
    model = SimpleDQN(state_size=6, n_actions=3).to(device)
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    
    # æº–å‚™æ•¸æ“š
    states = np.array([s for s, a in expert_data])
    actions = np.array([a for s, a in expert_data])
    
    states = torch.FloatTensor(states).to(device)
    actions = torch.LongTensor(actions).to(device)
    
    print(f"\nğŸ“ é–‹å§‹é è¨“ç·´ï¼ˆæ¨¡ä»¿å­¸ç¿’ï¼‰...")
    print(f"  è¨“ç·´æ¨£æœ¬: {len(expert_data)}")
    print(f"  Epochs: {n_epochs}")
    
    batch_size = 256
    n_batches = len(expert_data) // batch_size
    
    for epoch in range(n_epochs):
        total_loss = 0
        correct = 0
        
        # éš¨æ©Ÿæ‰“äº‚æ•¸æ“š
        indices = torch.randperm(len(expert_data))
        
        for i in range(n_batches):
            batch_indices = indices[i * batch_size:(i + 1) * batch_size]
            batch_states = states[batch_indices]
            batch_actions = actions[batch_indices]
            
            # å‰å‘å‚³æ’­
            outputs = model(batch_states)
            loss = criterion(outputs, batch_actions)
            
            # åå‘å‚³æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            total_loss += loss.item()
            
            # è¨ˆç®—æº–ç¢ºç‡
            _, predicted = torch.max(outputs, 1)
            correct += (predicted == batch_actions).sum().item()
        
        avg_loss = total_loss / n_batches
        accuracy = 100 * correct / (n_batches * batch_size)
        
        print(f"  Epoch {epoch + 1}/{n_epochs} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%")
    
    print(f"\nâœ“ é è¨“ç·´å®Œæˆï¼")
    
    return model


def test_pretrained_model(model, n_episodes=10):
    """æ¸¬è©¦é è¨“ç·´æ¨¡å‹çš„è¡¨ç¾"""
    device = get_device()
    env = gym.make("ALE/Pong-v5", render_mode=None)
    actions = [0, 2, 3]
    
    episode_rewards = []
    
    print(f"\nğŸ® æ¸¬è©¦é è¨“ç·´æ¨¡å‹...")
    
    for episode in range(n_episodes):
        obs, _ = env.reset()
        prev_ram = None
        state = extract_pong_state_from_ram(obs, prev_ram)
        
        episode_reward = 0
        done = False
        
        while not done:
            # ä½¿ç”¨æ¨¡å‹é¸æ“‡å‹•ä½œ
            with torch.no_grad():
                state_t = torch.FloatTensor(state).unsqueeze(0).to(device)
                q_values = model(state_t)
                action_idx = q_values.argmax().item()
                action = actions[action_idx]
            
            next_obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            
            next_state = extract_pong_state_from_ram(next_obs, obs)
            
            episode_reward += reward
            prev_ram = obs
            obs = next_obs
            state = next_state
        
        episode_rewards.append(episode_reward)
    
    env.close()
    
    avg_reward = np.mean(episode_rewards)
    print(f"  é è¨“ç·´æ¨¡å‹å¹³å‡çå‹µ: {avg_reward:.2f}")
    
    return avg_reward


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--debug', action='store_true', help='Enable debug output')
    args = parser.parse_args()
    
    print("=" * 60)
    print("Pong é è¨“ç·´ï¼šä½¿ç”¨è¦å‰‡ä»£ç†åˆå§‹åŒ–æ¨¡å‹")
    print("=" * 60)
    
    # æ­¥é©Ÿ 1ï¼šæ”¶é›†å°ˆå®¶æ•¸æ“š
    expert_data, rule_reward = collect_expert_data(n_episodes=50, debug=args.debug)
    
    # æ­¥é©Ÿ 2ï¼šé è¨“ç·´æ¨¡å‹
    model = pretrain_from_expert(expert_data, n_epochs=20)
    
    # æ­¥é©Ÿ 3ï¼šæ¸¬è©¦é è¨“ç·´æ¨¡å‹
    pretrain_reward = test_pretrained_model(model, n_episodes=10)
    
    # æ­¥é©Ÿ 4ï¼šä¿å­˜é è¨“ç·´æ¨¡å‹
    torch.save(model.state_dict(), 'pong_pretrained.pth')
    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜åˆ° 'pong_pretrained.pth'")
    
    # ç¸½çµ
    print("\n" + "=" * 60)
    print("ğŸ“Š ç¸½çµ")
    print("=" * 60)
    print(f"  è¦å‰‡ä»£ç†çå‹µ:     {rule_reward:.2f}")
    print(f"  é è¨“ç·´æ¨¡å‹çå‹µ:   {pretrain_reward:.2f}")
    
    if rule_reward > -20:
        print(f"\nâœ… é è¨“ç·´æˆåŠŸï¼è¦å‰‡ä»£ç†è¡¨ç¾è‰¯å¥½")
        print(f"ğŸ’¡ ç¾åœ¨å¯ä»¥é‹è¡Œ: python train.py --pretrain")
    else:
        print(f"\nâš ï¸  è¦å‰‡ä»£ç†è¡¨ç¾ä¸ä½³ï¼ˆ{rule_reward:.2f}ï¼‰")
        print(f"ğŸ’¡ å»ºè­°ç›´æ¥é‹è¡Œ: python train.py")
    
    print("=" * 60)


if __name__ == "__main__":
    main()