import gymnasium as gym
import ale_py
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from model import SimpleDQN, ReplayBuffer, get_device, extract_pong_state_from_ram

# è¨»å†Š ALE ç’°å¢ƒ
gym.register_envs(ale_py)

class PongTrainer:
    def __init__(self, env_name="ALE/Pong-v5", debug=False, pretrain_path=None):
        self.device = get_device()
        self.debug = debug
        
        # å‰µå»ºç’°å¢ƒ
        self.env = gym.make(env_name, render_mode=None)
        
        # ç°¡åŒ–çš„å‹•ä½œç©ºé–“ï¼šåªç”¨ STAY, UP, DOWN
        self.actions = [0, 2, 3]  # 0=NOOP, 2=UP, 3=DOWN
        self.n_actions = len(self.actions)
        self.state_size = 6  # [ball_x, ball_y, ball_vx, ball_vy, paddle_y, enemy_paddle_y]
        
        # è¶…åƒæ•¸ï¼ˆå„ªåŒ–éï¼‰
        self.batch_size = 64
        self.gamma = 0.99
        self.epsilon_start = 1.0
        self.epsilon_end = 0.1  # æé«˜æœ€ä½æ¢ç´¢ç‡
        self.epsilon_decay = 20000  # æ”¾æ…¢è¡°æ¸›
        self.target_update = 1000  # é™ä½æ›´æ–°é »ç‡
        self.learning_rate = 0.0003  # é™ä½å­¸ç¿’ç‡
        self.replay_capacity = 50000
        self.min_replay_size = 1000
        
        # å‰µå»ºç¶²çµ¡
        self.policy_net = SimpleDQN(self.state_size, self.n_actions).to(self.device)
        self.target_net = SimpleDQN(self.state_size, self.n_actions).to(self.device)
        
        # è¼‰å…¥é è¨“ç·´æ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰
        if pretrain_path:
            try:
                self.policy_net.load_state_dict(torch.load(pretrain_path, map_location=self.device))
                print(f"âœ“ è¼‰å…¥é è¨“ç·´æ¨¡å‹: {pretrain_path}")
            except FileNotFoundError:
                print(f"âš ï¸  æ‰¾ä¸åˆ°é è¨“ç·´æ¨¡å‹ {pretrain_path}ï¼Œå°‡å¾é›¶é–‹å§‹è¨“ç·´")
        
        self.target_net.load_state_dict(self.policy_net.state_dict())
        
        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)
        self.replay_buffer = ReplayBuffer(self.replay_capacity)
        
        self.steps = 0
        self.prev_ram = None
        
    def get_epsilon(self):
        """è¨ˆç®—ç•¶å‰çš„ epsilon å€¼"""
        return self.epsilon_end + (self.epsilon_start - self.epsilon_end) * \
               np.exp(-self.steps / self.epsilon_decay)
    
    def select_action(self, state):
        """ä½¿ç”¨ epsilon-greedy ç­–ç•¥é¸æ“‡å‹•ä½œ"""
        epsilon = self.get_epsilon()
        
        if np.random.random() < epsilon:
            return np.random.randint(self.n_actions)
        else:
            with torch.no_grad():
                state_t = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                q_values = self.policy_net(state_t)
                return q_values.argmax().item()
    
    def shape_reward(self, reward, state, next_state, done):
        """
        çå‹µå¡‘å½¢ï¼šçµ¦äºˆä¸­é–“çå‹µä»¥åŠ é€Ÿå­¸ç¿’ï¼ˆèª¿æ•´å¾Œçš„ç‰ˆæœ¬ï¼‰
        """
        shaped_reward = reward * 5  # æ¸›å°‘æ”¾å¤§å€æ•¸
        
        if not done and reward == 0:  # åªåœ¨æ²’å¾—åˆ†æ™‚çµ¦äºˆå¡‘å½¢çå‹µ
            # çå‹µï¼šçƒæ‹é è¿‘çƒ
            ball_y = state[1]
            paddle_y = state[4]
            next_ball_y = next_state[1]
            next_paddle_y = next_state[4]
            
            prev_dist = abs(ball_y - paddle_y)
            next_dist = abs(next_ball_y - next_paddle_y)
            
            # å¦‚æœçƒæ‹é è¿‘çƒï¼Œçµ¦äºˆå°çå‹µï¼ˆæ¸›å°å¹…åº¦ï¼‰
            if next_dist < prev_dist:
                shaped_reward += 0.05
            
            # è¼•å¾®æ‡²ç½°è·é›¢å¤ªé ï¼ˆæ¸›å°æ‡²ç½°ï¼‰
            if next_dist > 0.3:
                shaped_reward -= 0.01
        
        return shaped_reward
    
    def optimize_model(self):
        """åŸ·è¡Œä¸€æ­¥å„ªåŒ–"""
        if len(self.replay_buffer) < self.batch_size:
            return None
        
        states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
        
        # è½‰æ›ç‚ºå¼µé‡
        states = torch.FloatTensor(np.array(states)).to(self.device)
        actions = torch.LongTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).to(self.device)
        next_states = torch.FloatTensor(np.array(next_states)).to(self.device)
        dones = torch.FloatTensor(dones).to(self.device)
        
        # è¨ˆç®—ç•¶å‰ Q å€¼
        current_q_values = self.policy_net(states).gather(1, actions.unsqueeze(1))
        
        # Double DQN: ä½¿ç”¨ policy network é¸æ“‡å‹•ä½œï¼Œtarget network è©•ä¼°
        with torch.no_grad():
            next_actions = self.policy_net(next_states).argmax(1)
            next_q_values = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()
            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values
        
        # è¨ˆç®—æå¤±
        loss = nn.SmoothL1Loss()(current_q_values.squeeze(), target_q_values)
        
        # å„ªåŒ–
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)
        self.optimizer.step()
        
        return loss.item()
    
    def train(self, n_episodes=1000, save_interval=50):
        """è¨“ç·´ DQN"""
        episode_rewards = []
        episode_raw_rewards = []
        best_avg_reward = -float('inf')
        
        print("\né–‹å§‹è¨“ç·´æ”¹é€²ç‰ˆ Pong DQN...")
        print("âœ“ æ”¹é€²çš„è¦–è¦ºç‹€æ…‹æå–")
        print("âœ“ çå‹µå¡‘å½¢ï¼ˆreward shapingï¼‰")
        print("âœ“ Double DQN\n")
        
        for episode in range(n_episodes):
            obs, _ = self.env.reset()
            self.prev_ram = obs
            state = extract_pong_state_from_ram(obs, None)
            
            episode_reward = 0
            raw_reward = 0
            done = False
            losses = []
            hits = 0  # è¨˜éŒ„æ“Šçƒæ¬¡æ•¸
            
            while not done:
                # é¸æ“‡å‹•ä½œ
                action_idx = self.select_action(state)
                action = self.actions[action_idx]
                
                # åŸ·è¡Œå‹•ä½œ
                next_obs, reward, terminated, truncated, _ = self.env.step(action)
                done = terminated or truncated
                
                # æå–ä¸‹ä¸€å€‹ç‹€æ…‹
                next_state = extract_pong_state_from_ram(next_obs, self.prev_ram)
                
                # çå‹µå¡‘å½¢
                shaped_reward = self.shape_reward(reward, state, next_state, done)
                
                # è¨˜éŒ„åŸå§‹çå‹µ
                raw_reward += reward
                if reward > 0:
                    hits += 1
                
                # å­˜å„²ç¶“é©—
                self.replay_buffer.push(state, action_idx, shaped_reward, next_state, done)
                
                episode_reward += shaped_reward
                self.steps += 1
                
                # è¨“ç·´æ¨¡å‹
                if len(self.replay_buffer) >= self.min_replay_size:
                    loss = self.optimize_model()
                    if loss is not None:
                        losses.append(loss)
                
                # æ›´æ–°ç›®æ¨™ç¶²çµ¡
                if self.steps % self.target_update == 0:
                    self.target_net.load_state_dict(self.policy_net.state_dict())
                
                self.prev_ram = obs
                obs = next_obs
                state = next_state
            
            episode_rewards.append(episode_reward)
            episode_raw_rewards.append(raw_reward)
            
            avg_reward = np.mean(episode_rewards[-100:])
            avg_raw_reward = np.mean(episode_raw_rewards[-100:])
            avg_loss = np.mean(losses) if losses else 0
            
            # æ‰“å°é€²åº¦
            if episode % 10 == 0:
                trend = ""
                if len(episode_raw_rewards) >= 20:
                    recent_20 = np.mean(episode_raw_rewards[-20:])
                    prev_20 = np.mean(episode_raw_rewards[-40:-20]) if len(episode_raw_rewards) >= 40 else recent_20
                    if recent_20 > prev_20 + 0.5:
                        trend = "ğŸ“ˆ"
                    elif recent_20 < prev_20 - 0.5:
                        trend = "ğŸ“‰"
                    else:
                        trend = "â¡ï¸"
                
                print(f"Ep {episode:4d} | "
                      f"Raw: {raw_reward:3.0f} | "
                      f"Shaped: {episode_reward:7.1f} | "
                      f"Avg Raw(100): {avg_raw_reward:6.2f} {trend} | "
                      f"Îµ: {self.get_epsilon():.3f} | "
                      f"Loss: {avg_loss:.4f} | "
                      f"Hits: {hits} | "
                      f"Buffer: {len(self.replay_buffer)}")
            
            # Debug æ¨¡å¼ï¼šé¡¯ç¤ºç‹€æ…‹ä¿¡æ¯
            if self.debug and episode % 50 == 0:
                print(f"\n  [Debug] Episode {episode} æœ€å¾Œä¸€å¹€ç‹€æ…‹:")
                print(f"    Ball: ({state[0]:.2f}, {state[1]:.2f}), V: ({state[2]:.3f}, {state[3]:.3f})")
                print(f"    Paddle: {state[4]:.2f}, Enemy: {state[5]:.2f}")
            
            # ä¿å­˜æ¨¡å‹ï¼ˆåŸºæ–¼åŸå§‹çå‹µï¼‰
            if episode % save_interval == 0 or avg_raw_reward > best_avg_reward:
                if avg_raw_reward > best_avg_reward:
                    best_avg_reward = avg_raw_reward
                    torch.save(self.policy_net.state_dict(), 'pong_best.pth')
                    print(f"  â­ æ–°çš„æœ€ä½³å¹³å‡åŸå§‹çå‹µ: {best_avg_reward:.2f}")
                torch.save(self.policy_net.state_dict(), f'pong_ep{episode}.pth')
        
        self.env.close()
        print(f"\nè¨“ç·´å®Œæˆï¼æœ€ä½³å¹³å‡åŸå§‹çå‹µ: {best_avg_reward:.2f}")
        
        # é¡¯ç¤ºè¨“ç·´æ›²ç·šå»ºè­°
        print("\nğŸ’¡ æç¤ºï¼šä½ å¯ä»¥ç”¨ä»¥ä¸‹æŒ‡ä»¤æ¸¬è©¦æ¨¡å‹ï¼š")
        print("   python run.py --model pong_best.pth")

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--debug', action='store_true', help='Enable debug mode')
    parser.add_argument('--episodes', type=int, default=1000, help='Number of episodes')
    parser.add_argument('--pretrain', action='store_true', help='Load pretrained model')
    parser.add_argument('--pretrain-path', type=str, default='pong_pretrained.pth', 
                        help='Path to pretrained model')
    args = parser.parse_args()
    
    pretrain_path = args.pretrain_path if args.pretrain else None
    
    trainer = PongTrainer(debug=args.debug, pretrain_path=pretrain_path)
    trainer.train(n_episodes=args.episodes)